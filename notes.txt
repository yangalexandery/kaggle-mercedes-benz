3: 0.62897
4: 0.63034
5: 0.63576
6: 0.66206	0.6833 (0, 1), 0.65874 (1, 0)

1: train 0.66558
	-0.68326
	-0.58334
	-0.60147
2: train 0.66676
	-0.68326
	-0.60113
3: train 0.66573

4: train 0.67377
	-0.68952
	-0.60590

5: 0.67477
	-0.68326
	-0.58352
	-0.63710
	-0.68600

6: 0.66802
	-0.68326
	-0.58368
	-0.60208
	-0.68600
0.56777 on LB

//todo: improve both stacking
//rely on cross-validation
// model_3 is overfitting

benchmark:
about -69.7

7: mean val: -69.12882

useless columns removed: -69.65583
control: -70.25364
features removed: -69.37655
both: -68.74543

8: .3/.3/.4 split with noX0 replacements
	-score 1: -69.25741
	-score 2: -69.00942
	-score 3: -69.41994
	-mix: -68.77852

(repeat)
9: .3/.3/.4 split with noX0 replacements
	-score 1: -69.31639
	-score 2: -69.72828
	-score 3: -70.52934
	-mix: -69.05767
	-LB: 0.55875

10: repeat
	-mix: -68.71256
		-R^2 0.5971
		-others 69.5-70.0

test with maxabsscaler:



11: same, with probing

12: with MaxAbsScaler, mix .2/.2/.3/.3
	CV: .575

13: same

MSE goal: break 70
(r^2 score)
Model 1 CV: 0.57087, 70.243
Model 2 CV: 0.56995, 70.790
Model 3 CV: 0.56932, 70.009
Model 4 CV: 0.57122, 69.686

14: CV: 0.5746, 69.541

15: OOF R2 CV: 0.57164, 69.047
	Model 1 OOF: 0.56663
	Model 2 OOF: 0.56745
	Model 3 OOF: 0.56995
	Model 4 OOF: 0.56829